# -*- coding: utf-8 -*-
"""Chatbot(Using Qwen).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rzKm3a-EmT8EW2e5C2tHUCFf_rGXX-Ds
"""

# Import necessary libraries
import json  # For handling JSON files (loading the chatbot's conversation tree)
import torch  # PyTorch library for deep learning models
import spacy  # SpaCy library for Natural Language Processing (NLP)
import json
import torch
import spacy
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
# Load the tokenizer and model

# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2-1.5B-Instruct")
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2-1.5B-Instruct")

# Load the JSON structure
with open('/content/Chatbot.json', 'r') as file:
    tree = json.load(file)

# Convert tree into a dictionary for quick access
node_map = {node["nodeId"]: node for node in tree}

# Load the NER model
nlp = spacy.load("en_core_web_sm")

# Load the sentiment analysis model
sentiment_pipeline = pipeline("sentiment-analysis")

def get_llm_response(prompt, user_input):
    """Generate a response from Qwen2-1.5B-Instruct."""

    # Format input text with user input and AI prompt
    input_text = f"{prompt}\nUser: {user_input}\nAI:"

    # Tokenize the input text for the model
    inputs = tokenizer(input_text, return_tensors="pt", padding=True, truncation=True)

    # Generate response from the model without computing gradients (to save memory)
    with torch.no_grad():
        output_ids = model.generate(
            inputs["input_ids"],          # Input token IDs
            attention_mask=inputs["attention_mask"],  # Attention mask to handle padding
            max_length=100                # Limit the response length
        )

    # Decode the model's output into readable text
    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)

    # Extract AI's response part from the generated text
    return response.split("AI:")[-1].strip()

def extract_entities(text):
    """Extract named entities (like names, locations, dates) from text using SpaCy's NER."""

    doc = nlp(text)  # Process text with SpaCy's NLP model

    # Extract entities as a list of tuples (entity text, entity label)
    entities = [(ent.text, ent.label_) for ent in doc.ents]

    print(f"Extracted entities: {entities}")  # Debugging: Print extracted entities

    return entities  # Return extracted entities

def analyze_sentiment(text):
    """Analyze the sentiment of the given text using a pre-trained sentiment analysis model."""

    result = sentiment_pipeline(text)  # Run sentiment analysis on text

    return result[0]['label']  # Return the sentiment label (e.g., "positive", "negative", "neutral")

def get_next_node(current_node, user_input):
    """Finds the next node based on LLM-evaluated conditions."""

    # Iterate over all possible edges (transitions) from the current node
    for edge in current_node["edges"]:
        condition = edge["condition"]  # Extract the condition for this edge

        # Use LLM to check if the user's input matches the condition
        prompt = f"Does the following user response match the condition: '{condition}'? Answer 'yes' or 'no'."
        llm_response = get_llm_response(prompt, user_input).lower()  # Convert response to lowercase for consistency

        # If the LLM confirms a match, return the corresponding target node
        if "yes" in llm_response:
            return node_map.get(edge["targetNodeId"])  # Retrieve the next node from the node map

    return None  # If no matching condition is found, return None

def chatbot():
    """Runs the chatbot using the tree-based flow."""

    # Find the root node (starting point of the conversation)
    current_node = next(node for node in tree if node.get("rootNode", False))

    while True:  # Continuous loop to keep the chatbot running
        print(f"Bot: {current_node['prompt']}")  # Display the bot's message
        user_input = input("You: ")  # Get user input

        # Extract named entities from user input (for additional context understanding)
        entities = extract_entities(user_input)

        # Analyze sentiment of the user's response (e.g., positive, negative, neutral)
        sentiment = analyze_sentiment(user_input)

        # Print extracted information for debugging and analysis
        print(f"Entities: {entities}, Sentiment: {sentiment}")

        # Determine the next node based on user input and decision tree logic
        next_node = get_next_node(current_node, user_input)

        if next_node:
            current_node = next_node  # Move to the next node in the conversation
        else:
            print("Bot: I didn't understand. Let's try again.")  # Handle unrecognized responses

# Start the chatbot
chatbot()